{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing all the libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import pprint\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Libraries for display\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from threading import Timer,Thread,Event\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set Spark Context\n",
    "sc=SparkContext(appName='TwitterAnalysis')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc =StreamingContext(sc,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set Up checkpoint and get other context\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "socket_stream =ssc.socketTextStream('172.31.38.183',7777)\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create named tuples\n",
    "from collections import namedtuple\n",
    "# Tweet\n",
    "fields = (\"user\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "# Trend\n",
    "fields_1 = (\"trends\",\"count\")\n",
    "Trend = namedtuple('Trend', fields_1)\n",
    "# Sentiment\n",
    "fields_2 = (\"sentiment\", \"count\" )\n",
    "Sentiment =namedtuple('Sentiment',fields_2)\n",
    "# Language\n",
    "fields_3 = (\"language\", \"count\" )\n",
    "Language =namedtuple('Language',fields_3)\n",
    "# Geo\n",
    "fields_4 =(\"geo\",\"count\" )\n",
    "Geo = namedtuple('Geo',fields_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the json object\n",
    "def get_json(myjson): \n",
    "    try:\n",
    "        json_object = json.loads(myjson)\n",
    "    except ValueError, e:\n",
    "        return False\n",
    "    return json_object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the coordinates:\n",
    "def get_coord2(post):\n",
    "    coord = tuple()\n",
    "    try:\n",
    "        if post['coordinates'] == None:\n",
    "            coord = post['place']['bounding_box']['coordinates']\n",
    "            coord = reduce(lambda agg, nxt: [agg[0] + nxt[0], agg[1] + nxt[1]], coord[0])\n",
    "            coord = tuple(map(lambda t: t / 4.0, coord))\n",
    "        else:\n",
    "            coord = tuple(post['coordinates']['coordinates'])\n",
    "    except TypeError:\n",
    "        coord=(0,0)\n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the updated count\n",
    "def updateTotalCount(currentCount, countState):\n",
    "    if countState is None:\n",
    "        countState = 0\n",
    "    return sum(currentCount, countState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global Variable Declaration\n",
    "\n",
    "# Current Time\n",
    "t0 = datetime.now()\n",
    "# Removing punctuation\n",
    "remove_spl_char_regex = re.compile('[%s]' % re.escape(string.punctuation)) # regex to remove special characters\n",
    "# To remove stopwords\n",
    "stopwords=[u'rt', u'que',u'amp',u'get',u're', u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']\n",
    "# Dictionary for languages\n",
    "lang_dict = {'en':'English', 'pt': 'Portuguese','de':'German','es':'Spanish','nl':'Dutch','ru':'Russia','ja':'Japanesse','zh-cn':'Chinese','ko':'Koreans','hi':'Hindi','fr':'French','cs':'Czech','ur':'Urdu','und':'Afgani','tr':'Turkish','it':'Italian','ar':'Arabic','el':'Greek','da':'Danish','fa':'Persian','pl':'Polish','ro':'Romanian','sv':'Swedish','th':'Thai','uk':'Ukrainian','bn':'Bengali','fi':'Finnish','fil':'Filipino','hu':'Hungarian','id':'Indonesian','vi':'Vietnamese','zh-tw':'china','msa':'Malay','he':'Hebrew','no':'Norwegian'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Return language\n",
    "def lang(lang):\n",
    "    try: \n",
    "# return the key value\n",
    "        return(lang_dict[lang])\n",
    "    except KeyError as e:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the country\n",
    "# Spaces will be Omiited E.g. United States will become UnitedStates\n",
    "# Any country where the total length of words > 2 , only initials will be returned\n",
    "#  E.g. Republic of China = ROC , Republic of Phillippines  = ROP\n",
    "def country(country):\n",
    "    new_country = ' '\n",
    "    country = country.encode('ascii', 'ignore')\n",
    "    words = country.split()\n",
    "    if len(words) > 2:\n",
    "        for word in words:\n",
    "            new_country  =  new_country + str(word[0])\n",
    "        country =new_country\n",
    "    country=country.replace(\" \", \"\")\n",
    "    return(country)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return word token\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    text = text.encode('ascii', 'ignore') #to decode\n",
    "# remove special characters and urls with ''\n",
    "    text=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = remove_spl_char_regex.sub(\" \",text)  # Remove special characters\n",
    "    text=text.lower()\n",
    "\n",
    "    for word in text.split():\n",
    "# Remove Stop Words\n",
    "#take only word greater than 2 \n",
    "        if word not in stopwords \\\n",
    "            and word not in string.punctuation \\\n",
    "            and len(word)> 2 \\\n",
    "            and word != '``':\n",
    "                tokens.append(word)\n",
    "    return tokens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Tweet polarity\n",
    "def get_tweet_sentiment(tokens):\n",
    "        '''\n",
    "        Utility function to classify sentiment of passed tweet\n",
    "        using textblob's sentiment method\n",
    "        '''\n",
    "        text = ' '.join(str(e) for e in tokens)\n",
    "        # create TextBlob object of passed tweet text\n",
    "        analysis = TextBlob(text)\n",
    "        # set sentiment\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the streaming data into a Dstream\n",
    "lines = socket_stream.window(30)\n",
    "\n",
    "# Main dstreams\n",
    "dstream_tweets=(lines.map(lambda post: get_json(post)).filter(lambda post: post != False)  # return json \n",
    "            .filter(lambda post: 'created_at' in post)   # see if it is valid \n",
    "               .map(lambda post: (post[\"user\"][\"screen_name\"],post[\"user\"][\"followers_count\"],post[\"lang\"],post[\"user\"][\"statuses_count\"],post[\"text\"])) \n",
    "           .filter(lambda tpl: tpl[4] != ' ').filter(lambda tpl:tpl[2] != ' ') ) # Check text and language are not spaces\n",
    "\n",
    "tokenized_tweets = (dstream_tweets.map(lambda tpl: (tpl[4],tokenize(tpl[4])))  # Get tokenized list \n",
    "              .map(lambda tpl:(tpl[0],tpl[1],get_tweet_sentiment(tpl[1]))))    # Map sentiment against each tweet\n",
    "\n",
    "              \n",
    "\n",
    "# Remove below comment if you want to visualize the main dstreams\n",
    "#dstream_tweets.pprint() \n",
    "#tokenized_tweets.pprint()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 5 languages\n",
    "language=(dstream_tweets.map(lambda x: lang(x[2])).filter(lambda x:x!=' ')   # pick up language and check for spaces\n",
    "       .map( lambda lan: (lan, 1 )).reduceByKey(lambda a, b: a + b)      # Word Count\n",
    "       .updateStateByKey(updateTotalCount)                              # Add to the previous count\n",
    "        .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))   # Sort in descending order\n",
    "        .map(lambda x:\"Popular Language: %s\\tCount: %s\" % (x[0],x[1]))  )   # for printing\n",
    "\n",
    "#Remove below comment if you want to visualize the dstream\n",
    "#language.pprint(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 20 influential personalities\n",
    "influential = (dstream_tweets.map(lambda rec: ( rec[0], rec[1] ))     # pick up user id and follower's count\n",
    "              .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))  # descending order of follower's count\n",
    "              .map(lambda x:\"Influential People: %s\\tFollower's Count: %s\" % (x[0],x[1])) )# for printing\n",
    "        \n",
    "# Remove comment if you want to visualize the dstream\n",
    "#influential.pprint(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top tweeting user\n",
    "\n",
    "# Note: For most active user, we have treaded a different path. \n",
    "\n",
    "# Instead of accumulating user_id over a period of 1 hr, we have chosen to take corresponding statuses count \n",
    "\n",
    "# Statuses Count - The number of Tweets (including retweets) issued by the user.\n",
    "\n",
    "# This will not be the most accurate real time ,but we belive it gives us a better indication of which user is \n",
    "# presently tweeting and also quite active\n",
    "\n",
    "top_tweeting_user = (dstream_tweets.map(lambda x: (x[0], x[3]))     # pick up user_id ,statuses count\n",
    "                .transform(lambda twu: twu.sortBy(lambda x: x[1], ascending=False))  # descending order of count\n",
    "                .map(lambda x:\"Top Tweeting User: %s\\tTweet Count: %s\" % (x[0],x[1]))) # for printing\n",
    "\n",
    "# Remove comment if you want to visualize the dstream\n",
    "#top_tweeting_user.pprint(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most popular tweets\n",
    "\n",
    "# In case retweeted status is not null, then the tweet is retweeted. if it is true, it contains the original text \n",
    "# and the retweet count\n",
    "\n",
    "popular_tweets = (lines.map(lambda post: get_json(post)).filter(lambda post: post != False)  # get json \n",
    "                 .filter(lambda post: 'created_at' in post)   # take only valid and complete tweet\n",
    "                 .filter(lambda post:'retweeted_status' in post )  # Check if retweeted status is not null\n",
    "                 .map(lambda post:(post[\"text\"],post[\"retweeted_status\"][\"retweet_count\"]))  # get tweet and count\n",
    "                 .filter(lambda post: post[1] > 0)   # make sure the retweet count is greater than zero\n",
    "                 .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))   # descending order of count\n",
    "                .map(lambda x:\"Most Retweeted: %s\\tRetweet Count: %s\" % (x[0],x[1])) ) # for printing\n",
    "\n",
    "# Remove comment if you want to visualize the dstream\n",
    "#popular_tweets.pprint(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 5 words\n",
    "\n",
    "# we already have a tokenized text, so picking out the most used word \n",
    "# Note: We are not accumulating the counts from the previous states, so no updateStateByKey is used\n",
    "words= (tokenized_tweets.flatMap(lambda x: x[1]).map( lambda word: (word.lower(), 1 )).reduceByKey(lambda a, b: a + b) \\\n",
    "       .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False)) \n",
    "       .map(lambda x:\"Top Word: %s\\tCount: %s\" % (x[0],x[1])))\n",
    "\n",
    "# Remove comment if you want to visualize the dstream\n",
    "#words.pprint(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 10 trends\n",
    "\n",
    "# look for hashtags\n",
    "\n",
    "trends = (dstream_tweets.flatMap(lambda x: x[4].split(\" \")).filter( lambda word: word.lower().startswith(\"#\"))  #hashtag\n",
    "        .map( lambda word: ( word.lower(), 1)).reduceByKeyAndWindow( lambda a, b: a + b ,60,30)  # reducing it over the window\n",
    "     .updateStateByKey(updateTotalCount)   # add to the previous count\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))   # descending order of count\n",
    "    .map(lambda x:\"Top Trend: %s\\tCount: %s\" % (x[0],x[1])) ) # for printing\n",
    "\n",
    "# Remove comment if you want to visualize the dstream\n",
    "#trends.pprint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tweet Overall Sentiments\n",
    "# already available under tokenized tweets\n",
    "sentiments = (tokenized_tweets.map(lambda x : (x[2],1))    # take the sentiments and do a wordcount\n",
    "            .reduceByKey(lambda a, b: a + b)  # three kays ( Neutral, Positive,Negative)\n",
    "            .updateStateByKey(updateTotalCount)   # add to previous count\n",
    "            .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))   # descending order of count\n",
    "            .map(lambda x:\"Top Sentiment: %s\\tCount: %s\" % (x[0],x[1])) )  # for printing\n",
    "        \n",
    "# Remove comment if you want to visualize the dstream\n",
    "#sentiments.pprint()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Top 7 countries \n",
    "# Please note that most (90%) of the times geo data is not available, so we are working on the ones which are provided\n",
    "\n",
    "# The above indicates that we are discarding 90% of tweets, so this is not real representation , hence it is treated \n",
    "\n",
    "# as seperate and not derived from any many dstream\n",
    "\n",
    "country_tweets  = lines.map(lambda post: get_json(post)).filter(lambda post: post != False) \\\n",
    "                .filter(lambda post: 'created_at' in post) \\\n",
    "                .filter(lambda post : post[\"place\"]!= None) \\\n",
    "                .map(lambda post: country(post[\"place\"][\"country\"])) \\\n",
    "                .filter(lambda con: con!='') \\\n",
    "                .map( lambda con: (con, 1 )).reduceByKey(lambda a, b: a + b) \\\n",
    "                .updateStateByKey(updateTotalCount) \\\n",
    "                .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False)) \n",
    "                \n",
    "\n",
    "# Remove comment if you want to visualize the dstream\n",
    "# country_tweets.pprint(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coordinates \n",
    "coordinates =lines.map(lambda post: get_json(post)).filter(lambda post: post != False) \\\n",
    "                .filter(lambda post: 'created_at' in post) \\\n",
    "                .filter(lambda post : post[\"place\"]!= None) \\\n",
    "                .map(lambda post: (get_coord2(post)[0],get_coord2(post)[1])) \\\n",
    "                        \n",
    "            \n",
    "# Remove comment if you want to visualize the dstream            \n",
    "#coordinates.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First pick the tuple items\n",
    "# Get the values in DF\n",
    "# Register that as temp table\n",
    "\n",
    "# Tweets table\n",
    "(dstream_tweets.map(lambda rec: Tweet( rec[0], rec[1] ) ) \n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) \n",
    "  .limit(10).registerTempTable(\"tweets\") ) ) \n",
    "\n",
    "# Trends table\n",
    "(dstream_tweets.flatMap(lambda x: x[4].split(\" \")).filter( lambda word: word.lower().startswith(\"#\")) \n",
    "        .map( lambda word: ( word.lower(), 1)).reduceByKeyAndWindow( lambda a, b: a + b ,60,30) \n",
    "     .updateStateByKey(updateTotalCount) \n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False)) \n",
    "    .map(lambda rec: Trend( rec[0], rec[1] ) )\n",
    "    .foreachRDD( lambda rdd: rdd.toDF().limit(10).registerTempTable(\"trends\") ) )\n",
    "\n",
    "# Top Users\n",
    "(dstream_tweets.map(lambda x: (x[0], x[3])).transform(lambda twu: twu.sortBy(lambda x: x[1], ascending=False))\n",
    " .map(lambda rec: Tweet( rec[0], rec[1] ) ).foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " .limit(10).registerTempTable(\"topUsers\") ))\n",
    "\n",
    "# Sentiments\n",
    "(tokenized_tweets.map(lambda x : (x[2],1)).reduceByKey(lambda a, b: a + b) \n",
    " .updateStateByKey(updateTotalCount).transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))\n",
    " .map(lambda rec: Sentiment( rec[0], rec[1] ) )\n",
    " .foreachRDD( lambda rdd: rdd.toDF().registerTempTable(\"sentiments\") ) )\n",
    "\n",
    "# Language\n",
    "(dstream_tweets.map(lambda x: lang(x[2])).filter(lambda x:x!=' ') \n",
    " .map( lambda lan: (lan, 1 )).reduceByKey(lambda a, b: a + b) \n",
    " .updateStateByKey(updateTotalCount) \n",
    " .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False)) \n",
    " .map(lambda rec: Language( rec[0], rec[1] ) )\n",
    " .foreachRDD( lambda rdd: rdd.toDF().limit(5).registerTempTable(\"languages\") ) )\n",
    "\n",
    "# Geo \n",
    "(country_tweets.map(lambda rec: Geo( rec[0], rec[1] ) )\n",
    ".foreachRDD( lambda rdd: rdd.toDF().limit(5).registerTempTable(\"geos\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregated Dstream\n",
    "# Contains only first element of each dstream's rdd\n",
    "Aggregated =(language.map(lambda x:x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0])\\\n",
    "            .union(influential.map(lambda x: x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0]) \\\n",
    "            .union(top_tweeting_user.map(lambda x: x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0]) \\\n",
    "            .union(words.map(lambda x: x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0]) \\\n",
    "            .union(trends.map(lambda x: x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0]) \\\n",
    "            .union(sentiments.map(lambda x: x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0]) \\\n",
    "            .union(country_tweets.map(lambda x: x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0]).map(lambda x:\"Top Geo: %s\\tTweet Volume: %s\" % (x[0],x[1]))\n",
    "            .union(popular_tweets.map(lambda x: x).transform(lambda rdd :rdd.zipWithIndex()).filter(lambda x: x[1]==0) \\\n",
    "            .map(lambda x:x[0])))))))) )\n",
    "\n",
    "# Comment this out in case printing any other dstream\n",
    "Aggregated.pprint() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for eternal thread class\n",
    "class perpetualTimer():   \n",
    "\n",
    "    def __init__(self,t,hFunction):\n",
    "    \n",
    "        self.t=t\n",
    "        self.hFunction = hFunction\n",
    "        self.thread = Timer(self.t,self.handle_function)\n",
    "\n",
    "    def handle_function(self):\n",
    "        self.hFunction()\n",
    "        self.thread = Timer(self.t,self.handle_function)\n",
    "        self.thread.start()\n",
    "\n",
    "    def start(self):\n",
    "        self.thread.start()\n",
    "\n",
    "    def cancel(self):\n",
    "        self.thread.cancel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the graph dynamically\n",
    "\n",
    "# The graph run on back of a thread and will refresh itself automatically\n",
    "\n",
    "# 6 barplots in total\n",
    "def createGraph():\n",
    "    \n",
    "    global t0\n",
    "    total_mins = 60\n",
    "    diff = (datetime.now() - t0)\n",
    "    mins =int(diff.total_seconds()/60)\n",
    "    # Running for an hour\n",
    "    if mins < total_mins: \n",
    "        try:\n",
    "    # Graphing \n",
    "            top_10_tweets = sqlContext.sql( 'Select user, count from tweets' )\n",
    "            top_10_df = top_10_tweets.toPandas()\n",
    "            top_10_trends = sqlContext.sql( 'Select trends, count from trends' )\n",
    "            top_10_trends_df = top_10_trends.toPandas()\n",
    "            top_10_users = sqlContext.sql( 'Select user, count from topUsers' )\n",
    "            top_10_users_df = top_10_users.toPandas()\n",
    "            top_sentiment =sqlContext.sql('Select sentiment,count from sentiments')\n",
    "            top_sentiment_df = top_sentiment.toPandas()\n",
    "            top_language =sqlContext.sql('Select language,count from languages')\n",
    "            top_language_df =top_language.toPandas()\n",
    "            top_geo =sqlContext.sql('Select geo,count from geos')\n",
    "            top_geo_df =top_geo.toPandas()\n",
    "            display.clear_output(wait=True)\n",
    "            fig,ax =sns.plt.subplots(figsize=(15,15),nrows=3,ncols=2)\n",
    "            sns.barplot( x=\"count\", y=\"user\", data=top_10_df,ax=ax[0][0]).set_title('User vs Followers Count', fontsize=10,color=\"r\",alpha=0.5)\n",
    "            sns.barplot( x=\"count\", y=\"trends\", data=top_10_trends_df,ax=ax[2][0]).set_title('Twitter Trend Plot', fontsize=10,color=\"r\",alpha=0.5)\n",
    "            sns.barplot( x=\"count\", y=\"user\", data=top_10_users_df,ax=ax[1][0]).set_title('User vs Tweets Count', fontsize=10,color=\"r\",alpha=0.5)\n",
    "            sns.barplot( x=\"sentiment\", y=\"count\", data=top_sentiment_df,ax=ax[0][1]).set_title('Tweets Sentiment', fontsize=10,color=\"r\",alpha=0.5)\n",
    "            sns.barplot( x=\"language\", y=\"count\", data=top_language_df,ax=ax[1][1]).set_title('Most Tweeted Language', fontsize=10,color=\"r\",alpha=0.5)\n",
    "            sns.barplot( x=\"geo\", y=\"count\", data=top_geo_df,ax=ax[2][1]).set_title('Tweeting Geos', fontsize=10,color=\"r\",alpha=0.5)  \n",
    "            sns.plt.show()\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        stop_streaming()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start Streaming and plotting\n",
    "ssc.start()\n",
    "# Start the graph function after 40 secs\n",
    "t =perpetualTimer(40,createGraph)\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stopping the stream\n",
    "def stop_streaming():\n",
    "        t.cancel()\n",
    "        ssc.stop()\n",
    "        print(\"Hard Stop\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
